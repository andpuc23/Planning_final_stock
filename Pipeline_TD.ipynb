{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eda2588",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "33b4ba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.prepare_data import prepare_short_data, prepare_full_data\n",
    "from environment_TD import Environment\n",
    "from datetime import timedelta\n",
    "from models.base_model import Base_model\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "afb5b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Iterator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3eaa7",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c5c6765",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = prepare_full_data('data/Data_RU.xlsx')\n",
    "env = Environment(data=df, stock_name='AFKS', initial_money=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24fe513",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = env.first_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0696bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_count = env.data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405604f",
   "metadata": {},
   "source": [
    "# TD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c5b524ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(nn.Module):\n",
    "    \"\"\"Actor nn model\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 actor_layer_size: int = 100, \n",
    "                 actor_in_size: int = 10, \n",
    "                 actor_out_size: int = 3, \n",
    "                ) -> None:\n",
    "        \"\"\"\n",
    "        Init method\n",
    "        :param actor_layer_size: hidden layer's size\n",
    "        :param constraints_linear: linear constraints\n",
    "        :param constraints: angle constraints\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            torch.nn.Linear(actor_in_size, actor_layer_size),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(actor_layer_size, actor_layer_size),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(actor_layer_size, actor_out_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Main forward method\n",
    "        :param x: (batch_size, 6) state's tensor\n",
    "        \"\"\"\n",
    "        x = self.actor(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CriticModel(nn.Module):\n",
    "    \"\"\"Critic nn model\"\"\"\n",
    "\n",
    "    def __init__(self, critic_layer_size: int = 70,\n",
    "                       critic_in_size: int = 10, \n",
    "                       critic_out_size: int = 1, \n",
    "                       scale_factor: int = -5000) -> None:\n",
    "        \"\"\"\n",
    "        Init method\n",
    "        :param critic_layer_size: hidden critic's size\n",
    "        :param scale_factor: scaling for model's output\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            torch.nn.Linear(critic_in_size, critic_layer_size),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(critic_layer_size, critic_layer_size),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(critic_layer_size, critic_out_size)\n",
    "        )\n",
    "        self.act = torch.nn.Tanh()\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Main forward method\n",
    "        :param x: (batch_size, 6) state's tensor\n",
    "        \"\"\"\n",
    "        x = self.critic(x)\n",
    "        x = -(x ** 2)\n",
    "        x = self.act(x)\n",
    "        return x * self.scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c9eb9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticTD(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic Temporal Difference model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actor ,\n",
    "                       critic, #: CriticModel, \n",
    "                       transition, #: CameraTransition,\n",
    "                       env,\n",
    "                       satellite_discount: float = .98) -> None:\n",
    "        \"\"\"\n",
    "        Init method\n",
    "        :param actor: Actor nn model\n",
    "        :param critic: Critic nn model\n",
    "        :param transition: Camera Transition model\n",
    "        :param satellite_discount: satellite discount for TD method\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.transition = transition\n",
    "        self.env = env\n",
    "        self.satellite_discount = satellite_discount\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, state: torch.Tensor, indexes) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Main forward method\n",
    "        :param state: (batch_size, 6) camera's state\n",
    "        :return: TD loss\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "            next_state, reward = self.transition(state, action, indexes, self.env)\n",
    "            td_target = reward + self.satellite_discount * self.critic(next_state)\n",
    "        value = self.critic(state)\n",
    "        return self.loss(value, td_target)\n",
    "\n",
    "    def parameters(self) -> Iterator[Parameter]:\n",
    "        return self.critic.parameters()\n",
    "\n",
    "\n",
    "class ActorImprovedValue(nn.Module):\n",
    "\n",
    "    def __init__(self, actor, #: ActorModel,\n",
    "                       critic, #: CriticModel, \n",
    "                       transition, #: CameraTransition,\n",
    "                       env,\n",
    "                       satellite_discount: float = .98) -> None:\n",
    "        \"\"\"\n",
    "        Init method\n",
    "        :param actor: Actor nn model\n",
    "        :param critic: Critic nn model\n",
    "        :param transition: Camera Transition model\n",
    "        :param satellite_discount: satellite discount for TD method\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.critic = critic\n",
    "        self.actor = actor\n",
    "        self.transition = env.transition\n",
    "        self.env = env\n",
    "        self.satellite_discount = satellite_discount\n",
    "\n",
    "    def forward(self, state, index_):\n",
    "        \"\"\"\n",
    "        Main forward method\n",
    "        :param state: (batch_size, 6) camera's state\n",
    "        :return: actor's improved value\n",
    "        \"\"\"\n",
    "        action = self.actor(state)\n",
    "        next_state, reward = self.transition(state, action, indexes, self.env)\n",
    "        improved_value = reward + self.satellite_discount * self.critic(next_state)\n",
    "        return -improved_value.mean()\n",
    "\n",
    "    def parameters(self) -> Iterator[Parameter]:\n",
    "        return self.actor.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b2be15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_transition(state, action, indexes, env):    \n",
    "    action = (torch.argmax(action) - 1).numpy()\n",
    "    new_state, reward = env.transition_batch(action, indexes)\n",
    "    return new_state, reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bae8ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "actor = ActorModel(actor_in_size=102).to(device)\n",
    "critic = CriticModel(critic_in_size=102).to(device)\n",
    "\n",
    "critic_temporal_difference = CriticTD(actor, critic, td_transition, env).to(device)\n",
    "actor_improved_value = ActorImprovedValue(actor, critic, td_transition, env).to(device)\n",
    "\n",
    "\n",
    "model = Base_model()\n",
    "# here we have to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c2a3f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_critic_kind = torch.optim.Adam\n",
    "optimizer_critic_parameters = {\n",
    "    'lr': 5e-6,\n",
    "    'weight_decay': 1e-5\n",
    "}\n",
    "\n",
    "optimizer_actor_kind = torch.optim.Adam\n",
    "optimizer_actor_parameters = {\n",
    "    \"lr\" : 1e-4,\n",
    "    \"weight_decay\" : 1e-5\n",
    "}\n",
    "\n",
    "critic_iterations = 2000\n",
    "critic_batch_size = 4000\n",
    "\n",
    "actor_iterations = 2000\n",
    "actor_batch_size = 2000\n",
    "\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6e47d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_state(batch_size: int,\n",
    "                     env,\n",
    "                     index_min: int = 10,\n",
    "                     index_max: int = 11, \n",
    "                     ):\n",
    "    \n",
    "    rand_indexes = np.random.randint(index_min, index_max, batch_size)\n",
    "    \n",
    "    X = []\n",
    "    \n",
    "    for index in rand_indexes:\n",
    "        state_dict = env.observation(index)\n",
    "        state_array = np.concatenate([state_dict['prices'].drop(['Date'], axis=1).values.flatten(), \n",
    "                                      np.array(state_dict['money']),\n",
    "                                      np.array(state_dict['stocks_num'])\n",
    "                                     ])\n",
    "        X.append(state_array)\n",
    "        \n",
    "    return torch.Tensor(X), torch.tensor(rand_indexes)\n",
    "\n",
    "\n",
    "def critic_epoch(optimizer: torch.optim.Optimizer,\n",
    "                 model: CriticTD, \n",
    "                 iterations: int,\n",
    "                 env,\n",
    "                 batch_size: int) -> List[float]:\n",
    "    losses = []\n",
    "    for iteration in tqdm(range(iterations), \"Critic epoch\"):\n",
    "        X, indexes = get_random_state(batch_size, env, index_max=env.data.shape[0]-1)\n",
    "        X, indexes = X.to(device), indexes.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model(X, indexes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    print(f\"Critic mean loss: {np.mean(losses)}\")\n",
    "    return losses\n",
    "\n",
    "def actor_epoch(optimizer: torch.optim.Optimizer,\n",
    "                 model: CriticTD, \n",
    "                 iterations: int, \n",
    "                 env,\n",
    "                 batch_size: int) -> List[float]:\n",
    "    values = []\n",
    "    for iteration in tqdm(range(iterations), \"Actor epoch\"):\n",
    "        X, indexes = get_random_state(batch_size, env, index_max=env.data.shape[0]-1)\n",
    "        X, indexes = X.to(device), indexes.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        improved_value = model(X, indexes)\n",
    "        improved_value.backward()\n",
    "        optimizer.step()\n",
    "        values.append(improved_value.detach().cpu().numpy())\n",
    "    print(f\"Actor mean value: {np.mean(values)}\")\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dc28e031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7282491b6e43f5aaa87c8bd3d1398a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Actor-Critic learning:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506ae3e873f945489125806b1ad97dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Critic epoch:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-b6442208ecfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Actor-Critic learning\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0moptimizer_critic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_critic_kind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_temporal_difference\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptimizer_critic_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     losses = np.array(critic_epoch(optimizer_critic,\n\u001b[0m\u001b[0;32m      6\u001b[0m                                    \u001b[0mcritic_temporal_difference\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                    \u001b[0mcritic_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-144-22654a939e16>\u001b[0m in \u001b[0;36mcritic_epoch\u001b[1;34m(optimizer, model, iterations, env, batch_size)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-140-06599c3cce5c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state, indexes)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mtd_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msatellite_discount\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-141-5724579e0617>\u001b[0m in \u001b[0;36mtd_transition\u001b[1;34m(state, action, indexes, env)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtd_transition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransition_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Skoltech\\Planning AI\\Final_Project\\Planning_final_stock\\environment.py\u001b[0m in \u001b[0;36mtransition_batch\u001b[1;34m(self, action, indexes)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;31m#self.money.append(self.money[-1] - action*self.data.iloc[index][self.stock_name + '_close'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mnew_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Skoltech\\Planning AI\\Final_Project\\Planning_final_stock\\environment.py\u001b[0m in \u001b[0;36mobservation_batch\u001b[1;34m(self, indexes)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mnew_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mnew_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\Skoltech\\Planning AI\\Final_Project\\Planning_final_stock\\environment.py\u001b[0m in \u001b[0;36mobservation_tensor\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoney\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoney\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mobservation_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer_actor = optimizer_actor_kind(actor_improved_value.parameters(), **optimizer_actor_parameters)\n",
    "\n",
    "for _ in tqdm(range(epochs), \"Actor-Critic learning\", leave=False):\n",
    "    optimizer_critic = optimizer_critic_kind(critic_temporal_difference.parameters(), **optimizer_critic_parameters)\n",
    "    losses = np.array(critic_epoch(optimizer_critic,\n",
    "                                   critic_temporal_difference,\n",
    "                                   critic_iterations,\n",
    "                                   env,\n",
    "                                   actor_batch_size))\n",
    "    actor_epoch(optimizer_actor,\n",
    "                actor_improved_value,\n",
    "                actor_iterations,\n",
    "                env,\n",
    "                actor_batch_size)\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a526646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8477b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3be02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e26a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4978e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for i in range(10, day_count):\n",
    "    obs = env.observation(i)\n",
    "    action = model.predict(obs)\n",
    "    env.transition(action, i)\n",
    "    if i % 100 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b645d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(9, day_count), env.money)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1bac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(9, day_count), env.stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec1b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
